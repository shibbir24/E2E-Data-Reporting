{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation on the Address table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level 1 transformation: connect to the bronze container to get the data\n",
    "#list all the available data in SalesLT folder in the bronze container\n",
    "\n",
    "# dbutils.fs.ls('mnt/bronze/SalesLT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show silver container currently. it should be empty since no files have been moved here.\n",
    "\n",
    "# dbutils.fs.ls('mnt/silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test transformation on only one table, the address table. take address.parquet file. input path is the path of the bronze container.\n",
    "\n",
    "# input_path = '/mnt/bronze/SalesLT/Address/Address.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a PySpark dataframe for the input file. pass input path as parameter to the pyspark dataframe function\n",
    "#dataframe is like a temporary view with a table structure which has all the schema specific to the source file\n",
    "\n",
    "# df = spark.read.format('parquet').load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display table structure, shows all the different columns in the address table.\n",
    "#when a dataframe is created in PySpark it is stored in a temporary location which is easy to modify later.\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the ModifiedDate column. Convert to a date format structure without the time details\n",
    "\n",
    "# from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "# from pyspark.sql.types import TimestampType\n",
    "\n",
    "# df = df.withColumn(\"ModifiedDate\", date_format(from_utc_timestamp(df[\"ModifiedDate\"].cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the address table after conducting transformations to ModifiedDate column. ModifiedDate has been converted to date format.\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql --magic command to switch to sql cell. this is just for example purposes\n",
    "\n",
    "# -- SELECT 1 AS column1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each loop to iterate through the bronze container to get directory name and append it to the table_name array\n",
    "table_name = []\n",
    "\n",
    "for i in dbutils.fs.ls('mnt/bronze/SalesLT'):\n",
    "    table_name.append(i.name.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the table_name array\n",
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all the transformed data into the silver container\n",
    "\n",
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "#for each loop iterates through the table_name array and generates the input_path for all the tables using the mount location for the bronze container\n",
    "for i in table_name:\n",
    "    path = '/mnt/bronze/SalesLT/' + i + '/' + i + '.parquet'\n",
    "    #after generating the path, load it as a data frame\n",
    "    df = spark.read.format('parquet').load(path)\n",
    "    #get all the columns from the data frame as a list\n",
    "    column = df.columns\n",
    "\n",
    "    #for each loop to iterate through the column names to check if any of the columns have a date value in it\n",
    "    for col in column:\n",
    "        if \"Date\" in col or \"date\" in col:\n",
    "            df = df.withColumn(col, date_format(from_utc_timestamp(df[col].cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "    #output_path points to the silver container\n",
    "    output_path = '/mnt/silver/SalesLT/' + i + '/'\n",
    "    #write the transformed data to the data lake using the output_path in delta format\n",
    "    #we write the data to the silver and gold container using delta format. currently only writing data into one schema structure, but in the future if the input data schema changes then the Delta format can easily handle that.\n",
    "    df.write.format('delta').mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display df of last item in the list\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
